import streamlit as st
import requests

# --- CONFIGURA√á√ÉO DA API ---
# Coloque sua chave de API do Hugging Face aqui.
# Para um projeto real, use o st.secrets para proteger sua chave!
HF_API_KEY = "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" # <--- SUBSTITUA PELA SUA CHAVE REAL

# URL do modelo que sabemos que est√° funcionando e √© de alta qualidade.
API_URL = "https://api-inference.huggingface.co/models/google/gemma-7b-it"

# Cabe√ßalho da requisi√ß√£o (igual ao do c√≥digo C#)
headers = {
    "Authorization": f"Bearer {HF_API_KEY}",
    "Content-Type": "application/json"
}

# --- FUN√á√ÉO PARA CHAMAR A API ---
# Esta fun√ß√£o envia o prompt para o Hugging Face e retorna a resposta da IA.
def obter_resposta_ia(prompt):
    """
    Envia um prompt para a API do Hugging Face e retorna a resposta do modelo.
    """
    # Corpo da requisi√ß√£o com os par√¢metros do modelo
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": 1024,  # Aumentei um pouco para respostas mais completas
            "temperature": 0.7,
            "repetition_penalty": 1.2
        }
    }
    
    try:
        # Faz a requisi√ß√£o POST
        response = requests.post(API_URL, headers=headers, json=payload, timeout=30)
        
        # Verifica se a requisi√ß√£o falhou e retorna uma mensagem de erro clara
        if response.status_code != 200:
            # Erro 503 √© comum quando o modelo est√° carregando
            if response.status_code == 503:
                return "O modelo de IA est√° carregando. Por favor, aguarde 20 segundos e tente novamente."
            return f"Erro ao contatar a API: {response.status_code} - {response.text}"
            
        # Extrai o texto gerado da resposta JSON
        result = response.json()
        generated_text = result[0].get('generated_text', "N√£o foi poss√≠vel obter uma resposta.")
        
        # Limpa a resposta, removendo o prompt que o modelo √†s vezes repete
        if generated_text.startswith(prompt):
            return generated_text[len(prompt):].strip()
            
        return generated_text

    except requests.exceptions.RequestException as e:
        return f"Erro de conex√£o: {e}"

# --- INTERFACE DO CHAT COM STREAMLIT ---

st.set_page_config(page_title="Assistente Financeiro", page_icon="üí∞")
st.title("Assistente Financeiro com IA ü§ñ")
st.caption("Um prot√≥tipo para o projeto da UC de Boas Pr√°ticas.")

# Inicializa o hist√≥rico do chat na "mem√≥ria" da sess√£o do Streamlit
# Isso garante que a conversa n√£o seja perdida a cada intera√ß√£o.
if "messages" not in st.session_state:
    st.session_state.messages = []
    # Adiciona uma mensagem de boas-vindas do assistente na primeira vez
    st.session_state.messages.append(
        {"role": "assistant", "content": "Ol√°! Sou seu assistente financeiro. Como posso te ajudar a organizar suas finan√ßas hoje?"}
    )

# Exibe todas as mensagens do hist√≥rico no cont√™iner do chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Captura a nova mensagem do usu√°rio no campo de input que fica no final da p√°gina
if prompt := st.chat_input("Digite sua pergunta sobre finan√ßas..."):
    # 1. Adiciona a mensagem do usu√°rio ao hist√≥rico e exibe na tela
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. Gera e exibe a resposta do assistente
    with st.chat_message("assistant"):
        # Mostra um indicador de "pensando..." enquanto espera a resposta da API
        with st.spinner("Analisando e gerando sua resposta..."):
            response = obter_resposta_ia(prompt)
            st.markdown(response)
    
    # 3. Adiciona a resposta do assistente ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})
